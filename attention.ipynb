{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI-Kipuwy0AE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e018c0-744d-4681-b8ff-4e9e651d8a2d"
      },
      "source": [
        "# Download the dataset\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF\" -O english_hindi.csv && rm -rf /tmp/cookies.txt"
      ],
      "id": "aI-Kipuwy0AE",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-15 05:54:37--  https://docs.google.com/uc?export=download&confirm=&id=1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.100, 142.250.141.138, 142.250.141.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/juaufom0bqjtru32rs3dp7i0omnm4d72/1shdldmq4nujajhtadbqkqvt73ncgtbb/1623736425000/04727882715947363558/03236179916224479565Z/1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF?e=download [following]\n",
            "--2021-06-15 05:54:38--  https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/juaufom0bqjtru32rs3dp7i0omnm4d72/1shdldmq4nujajhtadbqkqvt73ncgtbb/1623736425000/04727882715947363558/03236179916224479565Z/1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF?e=download\n",
            "Resolving doc-0g-5g-docs.googleusercontent.com (doc-0g-5g-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-0g-5g-docs.googleusercontent.com (doc-0g-5g-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=j97h726vjuqdu&continue=https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/juaufom0bqjtru32rs3dp7i0omnm4d72/1shdldmq4nujajhtadbqkqvt73ncgtbb/1623736425000/04727882715947363558/03236179916224479565Z/1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF?e%3Ddownload&hash=k844iv1e2fb0v4u3567q8sfpsq1hm1vh [following]\n",
            "--2021-06-15 05:54:38--  https://docs.google.com/nonceSigner?nonce=j97h726vjuqdu&continue=https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/juaufom0bqjtru32rs3dp7i0omnm4d72/1shdldmq4nujajhtadbqkqvt73ncgtbb/1623736425000/04727882715947363558/03236179916224479565Z/1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF?e%3Ddownload&hash=k844iv1e2fb0v4u3567q8sfpsq1hm1vh\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/juaufom0bqjtru32rs3dp7i0omnm4d72/1shdldmq4nujajhtadbqkqvt73ncgtbb/1623736425000/04727882715947363558/03236179916224479565Z/1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF?e=download&nonce=j97h726vjuqdu&user=03236179916224479565Z&hash=5cuvf1v9jejmvau2rgoonlvkgih7hgoe [following]\n",
            "--2021-06-15 05:54:38--  https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/juaufom0bqjtru32rs3dp7i0omnm4d72/1shdldmq4nujajhtadbqkqvt73ncgtbb/1623736425000/04727882715947363558/03236179916224479565Z/1GeIz3GMuWIRTi_BtW6cYy39O0Mu3ujRF?e=download&nonce=j97h726vjuqdu&user=03236179916224479565Z&hash=5cuvf1v9jejmvau2rgoonlvkgih7hgoe\n",
            "Connecting to doc-0g-5g-docs.googleusercontent.com (doc-0g-5g-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘english_hindi.csv’\n",
            "\n",
            "english_hindi.csv       [   <=>              ]  39.48M  65.4MB/s    in 0.6s    \n",
            "\n",
            "2021-06-15 05:54:39 (65.4 MB/s) - ‘english_hindi.csv’ saved [41399186]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a32dbc87"
      },
      "source": [
        "# import necessary files\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from string import digits\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import GRU, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "a32dbc87",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4273e86"
      },
      "source": [
        "## Preprocess"
      ],
      "id": "e4273e86"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bc2e533"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD',\n",
        "                                                   s))\n",
        "def preprocess_sentence(s, hindi=False):\n",
        "    punctuations = string.punctuation\n",
        "    digits = string.digits\n",
        "    remove_digits = str.maketrans('', '', digits)\n",
        "    \n",
        "    # convert to lowercase\n",
        "    s = s.lower()\n",
        "    s = s.translate(remove_digits)\n",
        "    \n",
        "    if not hindi:\n",
        "        # convert to ascii\n",
        "        s = unicode_to_ascii(s.strip())\n",
        "    else:\n",
        "        s = re.sub(\"[२३०८१५७९४६]\", \"\", s)\n",
        "        \n",
        "    # remove punctuations\n",
        "    s = \"\".join([c for c in s if c not in punctuations])\n",
        "    \n",
        "    # remove extra spaces\n",
        "    s = s.strip()\n",
        "    \n",
        "    return s\n",
        "    "
      ],
      "id": "7bc2e533",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "a7d3000a",
        "outputId": "5a28ab7a-ea10-4fe0-be61-51dcc1a655c3"
      },
      "source": [
        "preprocess_sentence(\"This is 100 dollars\")"
      ],
      "id": "a7d3000a",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is  dollars'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78572f8a"
      },
      "source": [
        "def create_dataset(path):\n",
        "    data = pd.read_csv(path)\n",
        "    data = data.dropna()\n",
        "    data = data[data[\"source\"].isin([\"ted\", \"tides\"])]\n",
        "    data[\"inp_len\"] = data[\"english_sentence\"].apply(lambda x: len(x.split()))\n",
        "    data = data[data[\"inp_len\"] <= 50]\n",
        "\n",
        "    \n",
        "    english_sentences, hindi_sentences = [], []\n",
        "    \n",
        "    for i, j in zip(data['english_sentence'], data[\"hindi_sentence\"]):\n",
        "        eng_sent = preprocess_sentence(i).split()\n",
        "        hin_sent = preprocess_sentence(j, hindi=True).split()\n",
        "        \n",
        "        eng_sent.append(\"<end>\")\n",
        "        eng_sent.insert(0, \"<start>\")\n",
        "        hin_sent.append(\"<end>\")\n",
        "        hin_sent.insert(0, \"<start>\")\n",
        "        english_sentences.append(eng_sent)\n",
        "        hindi_sentences.append(hin_sent)\n",
        "        \n",
        "    return english_sentences, hindi_sentences"
      ],
      "id": "78572f8a",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd9c04a1",
        "outputId": "67b45f25-3ad8-49ca-a2bd-026d4badc262"
      },
      "source": [
        "english_sentences, hindi_sentences = create_dataset(path)\n",
        "len(english_sentences), len(hindi_sentences)"
      ],
      "id": "bd9c04a1",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87776, 87776)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd7f470e",
        "outputId": "5eef81cc-7421-4181-8470-8c8e2fcebac8"
      },
      "source": [
        "print(\"Source: {}\\nTarget: {}\".format(english_sentences[0],\n",
        "                                      hindi_sentences[0]))"
      ],
      "id": "dd7f470e",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source: ['<start>', 'politicians', 'do', 'not', 'have', 'permission', 'to', 'do', 'what', 'needs', 'to', 'be', 'done', '<end>']\n",
            "Target: ['<start>', 'राजनीतिज्ञों', 'के', 'पास', 'जो', 'कार्य', 'करना', 'चाहिए', 'वह', 'करने', 'कि', 'अनुमति', 'नहीं', 'है', '<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ba789d"
      },
      "source": [
        "## Tokenize the data"
      ],
      "id": "c0ba789d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b65fd06e"
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = keras.preprocessing.sequence.pad_sequences(\n",
        "        tensor, padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "id": "b65fd06e",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99a2ae3e"
      },
      "source": [
        "def load_dataset(path):\n",
        "    source_lang, target_lang = create_dataset(path)\n",
        "    input_tensor, input_tokenizer = tokenize(source_lang)\n",
        "    target_tensor, target_tokenizer = tokenize(target_lang)\n",
        "    return input_tensor, target_tensor, input_tokenizer, target_tokenizer\n"
      ],
      "id": "99a2ae3e",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f532be7",
        "outputId": "8a05beef-dc6f-4400-de03-ce5288bbe1bf"
      },
      "source": [
        "# load dataset\n",
        "input_tensor, target_tensor, input_tokenizer, target_tokenizer = load_dataset(path)\n",
        "max_input_len = max([len(x) for x in input_tensor])\n",
        "max_target_len = max([len(x) for x in target_tensor])\n",
        "\n",
        "max_input_len, max_target_len"
      ],
      "id": "4f532be7",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51, 95)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf1c687d"
      },
      "source": [
        "## Create train and test dataset"
      ],
      "id": "cf1c687d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "065b5ac3",
        "outputId": "557b677a-2e90-4587-fb16-37fe46f6b3b4"
      },
      "source": [
        "input_tensor_train, input_tensor_val, \\\n",
        "target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor,\n",
        "                                                          test_size=0.2, random_state=42)\n",
        "input_tensor_train.shape, input_tensor_val.shape, \\\n",
        "target_tensor_train.shape, target_tensor_val.shape"
      ],
      "id": "065b5ac3",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((70220, 51), (17556, 51), (70220, 95), (17556, 95))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "045cee6d"
      },
      "source": [
        "## Create TensorFlow Dataset"
      ],
      "id": "045cee6d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fc73772"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_size_input = len(input_tokenizer.word_index)+1\n",
        "vocab_size_target = len(target_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,\n",
        "                                              target_tensor_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "id": "2fc73772",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e697ea1"
      },
      "source": [
        "## Build the Model"
      ],
      "id": "2e697ea1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e788959"
      },
      "source": [
        "### Encoder"
      ],
      "id": "3e788959"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a51ed01e"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units,\n",
        "                batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = GRU(self.enc_units,\n",
        "                       return_sequences=True,\n",
        "                       return_state=True,\n",
        "                       recurrent_initializer=\"glorot_uniform\")\n",
        "    \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.enc_units))\n",
        "    \n",
        "encoder = Encoder(vocab_size_input, embedding_dim, units, BATCH_SIZE)"
      ],
      "id": "a51ed01e",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42f06427"
      },
      "source": [
        "### Attention Mechanism (BahdaunauAttention)"
      ],
      "id": "42f06427"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f73c1c85"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, query, values):\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights"
      ],
      "id": "f73c1c85",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06a3a15d"
      },
      "source": [
        "### Decoder"
      ],
      "id": "06a3a15d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ca3ce59"
      },
      "source": [
        "class Decoder(keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units,\n",
        "                 batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = GRU(dec_units,\n",
        "                       return_sequences=True,\n",
        "                       return_state=True,\n",
        "                       recurrent_initializer=\"glorot_uniform\")\n",
        "        self.fc = keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden,\n",
        "                                                           enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights\n",
        "    \n",
        "decoder = Decoder(vocab_size_target, embedding_dim, units, BATCH_SIZE)"
      ],
      "id": "1ca3ce59",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4d1b36d"
      },
      "source": [
        "## Define optimizer and loss function"
      ],
      "id": "a4d1b36d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0349ac4"
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                         reduction='none')\n",
        "\n",
        "def loss_function(true, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
        "    loss_ = loss_object(true, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "id": "d0349ac4",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "add077c4"
      },
      "source": [
        "# Create checkpoints\n",
        "checkpoint_dir = \"/content/drive/MyDrive/eng_hindi/checkpoint2\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "id": "add077c4",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05a9ac24"
      },
      "source": [
        "## Training Model"
      ],
      "id": "05a9ac24"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c033d2e3"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']]*BATCH_SIZE, 1)\n",
        "        \n",
        "        # teacher forcing method\n",
        "        for i in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, i], 1)\n",
        "            \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        return batch_loss\n",
        "    "
      ],
      "id": "c033d2e3",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abcde7e4",
        "outputId": "41e8978f-c8fc-4b7c-fde0-c950acb2e415"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        if batch % 100 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch+1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "            \n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "    print(\"Epoch {} Loss {:.4f}\".format(epoch+1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print(\"Time taken for 1 epoch {} sec\\n\".format(time.time() - start))"
      ],
      "id": "abcde7e4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0136\n",
            "Epoch 1 Batch 100 Loss 1.1946\n",
            "Epoch 1 Batch 200 Loss 1.2330\n",
            "Epoch 1 Batch 300 Loss 1.2124\n",
            "Epoch 1 Batch 400 Loss 1.3548\n",
            "Epoch 1 Batch 500 Loss 1.2309\n",
            "Epoch 1 Batch 600 Loss 1.2322\n",
            "Epoch 1 Batch 700 Loss 1.3354\n",
            "Epoch 1 Batch 800 Loss 1.2038\n",
            "Epoch 1 Batch 900 Loss 1.1221\n",
            "Epoch 1 Batch 1000 Loss 1.2161\n",
            "Epoch 1 Loss 1.2186\n",
            "Time taken for 1 epoch 806.9351568222046 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.1897\n",
            "Epoch 2 Batch 100 Loss 1.0392\n",
            "Epoch 2 Batch 200 Loss 1.0479\n",
            "Epoch 2 Batch 300 Loss 1.0433\n",
            "Epoch 2 Batch 400 Loss 1.1533\n",
            "Epoch 2 Batch 500 Loss 0.9413\n",
            "Epoch 2 Batch 600 Loss 1.0643\n",
            "Epoch 2 Batch 700 Loss 1.0075\n",
            "Epoch 2 Batch 800 Loss 1.0305\n",
            "Epoch 2 Batch 900 Loss 1.1263\n",
            "Epoch 2 Batch 1000 Loss 0.9490\n",
            "Epoch 2 Loss 1.0888\n",
            "Time taken for 1 epoch 688.0440459251404 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.8660\n",
            "Epoch 3 Batch 100 Loss 0.9653\n",
            "Epoch 3 Batch 200 Loss 0.9599\n",
            "Epoch 3 Batch 300 Loss 0.8560\n",
            "Epoch 3 Batch 400 Loss 1.1662\n",
            "Epoch 3 Batch 500 Loss 1.0913\n",
            "Epoch 3 Batch 600 Loss 1.1185\n",
            "Epoch 3 Batch 700 Loss 1.0856\n",
            "Epoch 3 Batch 800 Loss 1.0522\n",
            "Epoch 3 Batch 900 Loss 1.0004\n",
            "Epoch 3 Batch 1000 Loss 1.0291\n",
            "Epoch 3 Loss 1.0074\n",
            "Time taken for 1 epoch 687.6203184127808 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0001\n",
            "Epoch 4 Batch 100 Loss 0.9819\n",
            "Epoch 4 Batch 200 Loss 1.0127\n",
            "Epoch 4 Batch 300 Loss 0.9844\n",
            "Epoch 4 Batch 400 Loss 0.8148\n",
            "Epoch 4 Batch 500 Loss 0.8995\n",
            "Epoch 4 Batch 600 Loss 0.9949\n",
            "Epoch 4 Batch 700 Loss 0.9148\n",
            "Epoch 4 Batch 800 Loss 0.8033\n",
            "Epoch 4 Batch 900 Loss 0.9718\n",
            "Epoch 4 Batch 1000 Loss 0.9293\n",
            "Epoch 4 Loss 0.9287\n",
            "Time taken for 1 epoch 689.0212936401367 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.8621\n",
            "Epoch 5 Batch 100 Loss 0.9311\n",
            "Epoch 5 Batch 200 Loss 0.8132\n",
            "Epoch 5 Batch 300 Loss 0.8300\n",
            "Epoch 5 Batch 400 Loss 0.8414\n",
            "Epoch 5 Batch 500 Loss 0.8358\n",
            "Epoch 5 Batch 600 Loss 0.8497\n",
            "Epoch 5 Batch 700 Loss 0.8394\n",
            "Epoch 5 Batch 800 Loss 0.8065\n",
            "Epoch 5 Batch 900 Loss 0.8767\n",
            "Epoch 5 Batch 1000 Loss 0.9278\n",
            "Epoch 5 Loss 0.8591\n",
            "Time taken for 1 epoch 688.1553268432617 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.6123\n",
            "Epoch 6 Batch 100 Loss 0.8385\n",
            "Epoch 6 Batch 200 Loss 0.7902\n",
            "Epoch 6 Batch 300 Loss 0.8314\n",
            "Epoch 6 Batch 400 Loss 0.7770\n",
            "Epoch 6 Batch 500 Loss 0.7740\n",
            "Epoch 6 Batch 600 Loss 0.9606\n",
            "Epoch 6 Batch 700 Loss 0.7395\n",
            "Epoch 6 Batch 800 Loss 0.7803\n",
            "Epoch 6 Batch 900 Loss 0.7826\n",
            "Epoch 6 Batch 1000 Loss 0.7123\n",
            "Epoch 6 Loss 0.7993\n",
            "Time taken for 1 epoch 689.146909236908 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7538\n",
            "Epoch 7 Batch 100 Loss 0.6996\n",
            "Epoch 7 Batch 200 Loss 0.7740\n",
            "Epoch 7 Batch 300 Loss 0.7383\n",
            "Epoch 7 Batch 400 Loss 0.7187\n",
            "Epoch 7 Batch 500 Loss 0.9112\n",
            "Epoch 7 Batch 600 Loss 0.8631\n",
            "Epoch 7 Batch 700 Loss 0.9052\n",
            "Epoch 7 Batch 800 Loss 0.7712\n",
            "Epoch 7 Batch 900 Loss 0.6931\n",
            "Epoch 7 Batch 1000 Loss 0.7178\n",
            "Epoch 7 Loss 0.7444\n",
            "Time taken for 1 epoch 688.1130795478821 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.6360\n",
            "Epoch 8 Batch 100 Loss 0.7161\n",
            "Epoch 8 Batch 200 Loss 0.6491\n",
            "Epoch 8 Batch 300 Loss 0.7148\n",
            "Epoch 8 Batch 400 Loss 0.6044\n",
            "Epoch 8 Batch 500 Loss 0.6589\n",
            "Epoch 8 Batch 600 Loss 0.7258\n",
            "Epoch 8 Batch 700 Loss 0.7513\n",
            "Epoch 8 Batch 800 Loss 0.7389\n",
            "Epoch 8 Batch 900 Loss 0.6916\n",
            "Epoch 8 Batch 1000 Loss 0.8934\n",
            "Epoch 8 Loss 0.6944\n",
            "Time taken for 1 epoch 688.275105714798 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.6936\n",
            "Epoch 9 Batch 100 Loss 0.5059\n",
            "Epoch 9 Batch 200 Loss 0.6289\n",
            "Epoch 9 Batch 300 Loss 0.6703\n",
            "Epoch 9 Batch 400 Loss 0.6671\n",
            "Epoch 9 Batch 500 Loss 0.6585\n",
            "Epoch 9 Batch 600 Loss 0.7074\n",
            "Epoch 9 Batch 700 Loss 0.6688\n",
            "Epoch 9 Batch 800 Loss 0.6620\n",
            "Epoch 9 Batch 900 Loss 0.6249\n",
            "Epoch 9 Batch 1000 Loss 0.8354\n",
            "Epoch 9 Loss 0.6483\n",
            "Time taken for 1 epoch 687.7746489048004 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.6685\n",
            "Epoch 10 Batch 100 Loss 0.6194\n",
            "Epoch 10 Batch 200 Loss 0.5699\n",
            "Epoch 10 Batch 300 Loss 0.7402\n",
            "Epoch 10 Batch 400 Loss 0.7237\n",
            "Epoch 10 Batch 500 Loss 0.6059\n",
            "Epoch 10 Batch 600 Loss 0.6772\n",
            "Epoch 10 Batch 700 Loss 0.5445\n",
            "Epoch 10 Batch 800 Loss 0.6313\n",
            "Epoch 10 Batch 900 Loss 0.5714\n",
            "Epoch 10 Batch 1000 Loss 0.5394\n",
            "Epoch 10 Loss 0.6068\n",
            "Time taken for 1 epoch 688.7285783290863 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6431\n",
            "Epoch 11 Batch 100 Loss 0.5869\n",
            "Epoch 11 Batch 200 Loss 0.4802\n",
            "Epoch 11 Batch 300 Loss 0.5481\n",
            "Epoch 11 Batch 400 Loss 0.6237\n",
            "Epoch 11 Batch 500 Loss 0.6240\n",
            "Epoch 11 Batch 600 Loss 0.6060\n",
            "Epoch 11 Batch 700 Loss 0.4587\n",
            "Epoch 11 Batch 800 Loss 0.5677\n",
            "Epoch 11 Batch 900 Loss 0.5148\n",
            "Epoch 11 Batch 1000 Loss 0.5731\n",
            "Epoch 11 Loss 0.5694\n",
            "Time taken for 1 epoch 687.9997296333313 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.5700\n",
            "Epoch 12 Batch 100 Loss 0.4901\n",
            "Epoch 12 Batch 200 Loss 0.5036\n",
            "Epoch 12 Batch 300 Loss 0.5809\n",
            "Epoch 12 Batch 400 Loss 0.4537\n",
            "Epoch 12 Batch 500 Loss 0.5993\n",
            "Epoch 12 Batch 600 Loss 0.5513\n",
            "Epoch 12 Batch 700 Loss 0.5571\n",
            "Epoch 12 Batch 800 Loss 0.5504\n",
            "Epoch 12 Batch 900 Loss 0.5699\n",
            "Epoch 12 Batch 1000 Loss 0.5323\n",
            "Epoch 12 Loss 0.5361\n",
            "Time taken for 1 epoch 688.1203684806824 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.4125\n",
            "Epoch 13 Batch 100 Loss 0.4207\n",
            "Epoch 13 Batch 200 Loss 0.4539\n",
            "Epoch 13 Batch 300 Loss 0.5799\n",
            "Epoch 13 Batch 400 Loss 0.6471\n",
            "Epoch 13 Batch 500 Loss 0.4996\n",
            "Epoch 13 Batch 600 Loss 0.5192\n",
            "Epoch 13 Batch 700 Loss 0.4502\n",
            "Epoch 13 Batch 800 Loss 0.5472\n",
            "Epoch 13 Batch 900 Loss 0.5888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecc6f5a5"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_target_len, max_input_len))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [input_tokenizer.word_index[i] for i in sentence.split()]\n",
        "    inputs = pad_sequences([inputs], maxlen=max_input_len, padding=\"post\")\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_output, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    for i in range(max_target_len):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden,\n",
        "                                                              enc_output)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        if target_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        result += target_tokenizer.index_word[predicted_id] + ' '\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence\n",
        "        "
      ],
      "id": "ecc6f5a5",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cb67551"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Translated {}\".format(result))"
      ],
      "id": "1cb67551",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O-tgwUMdb6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4926a6-5648-48b7-8e96-cb1f769bd606"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "id": "3O-tgwUMdb6m",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1d8f8b4850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c50884c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ad2366-16e3-40bb-edc9-1a2b6821df6f"
      },
      "source": [
        "translate(u'politicians do not have permission to do what needs to be done.')"
      ],
      "id": "c50884c0",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: politicians do not have permission to do what needs to be done\n",
            "Translated वे नेता नहीं हैं कि वे क्या करना चाहिए \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c2ece07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078399c7-24ca-4682-c52d-6e4522b83f8c"
      },
      "source": [
        "translate(u\"where are you from?\")"
      ],
      "id": "6c2ece07",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: where are you from\n",
            "Translated जहां आप \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64946e15"
      },
      "source": [
        ""
      ],
      "id": "64946e15",
      "execution_count": null,
      "outputs": []
    }
  ]
}