{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a32dbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary files\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import digits\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8b290",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51267c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted         I'd like to tell you about one such child,   \n",
       "2  indic2012  This percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they're bad at not...   \n",
       "4  indic2012  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/Hindi_English_Truncated_Corpus.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d28d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95605f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39881, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract data from ted only\n",
    "ted_data = data[data['source']=='ted']\n",
    "ted_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4273e86",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc2e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',\n",
    "                                                   s))\n",
    "def preprocess_sentence(s, hindi=False):\n",
    "    punctuations = string.punctuation\n",
    "    digits = string.digits\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    s = s.lower()\n",
    "    if not hindi:\n",
    "        \n",
    "        # convert to ascii\n",
    "        s = unicode_to_ascii(s.strip())\n",
    "#     s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s )\n",
    "#     s = re.sub(r'[\" \"]+', \" \", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "#     s = s.rstrip().strip()\n",
    "        # remove digits\n",
    "        s = s.translate(remove_digits)\n",
    "    else:\n",
    "        s = re.sub(\"[२३०८१५७९४६]\", \"\", s)\n",
    "        \n",
    "    # remove punctuations\n",
    "    s = \"\".join([c for c in s if c not in punctuations])\n",
    "    \n",
    "    # remove extra spaces\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d3000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is  dollars'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"This is 100 dollars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78572f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.dropna()\n",
    "    data = data[data[\"source\"] == \"ted\"]\n",
    "    \n",
    "    english_sentences, hindi_sentences = [], []\n",
    "    \n",
    "    for i, j in zip(data['english_sentence'], data[\"hindi_sentence\"]):\n",
    "        eng_sent = preprocess_sentence(i).split()\n",
    "        hin_sent = preprocess_sentence(j, hindi=True).split()\n",
    "        \n",
    "        eng_sent.append(\"<end>\")\n",
    "        eng_sent.insert(0, \"<start>\")\n",
    "        hin_sent.append(\"<end>\")\n",
    "        hin_sent.insert(0, \"<start>\")\n",
    "        english_sentences.append(eng_sent)\n",
    "        hindi_sentences.append(hin_sent)\n",
    "        \n",
    "    return english_sentences, hindi_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9c04a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39881, 39881)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences, hindi_sentences = create_dataset('data/Hindi_English_Truncated_Corpus.csv')\n",
    "len(english_sentences), len(hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7f470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: ['<start>', 'politicians', 'do', 'not', 'have', 'permission', 'to', 'do', 'what', 'needs', 'to', 'be', 'done', '<end>']\n",
      "Target: ['<start>', 'राजनीतिज्ञों', 'के', 'पास', 'जो', 'कार्य', 'करना', 'चाहिए', 'वह', 'करने', 'कि', 'अनुमति', 'नहीं', 'है', '<end>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: {}\\nTarget: {}\".format(english_sentences[0],\n",
    "                                      hindi_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba789d",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b65fd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(\n",
    "        tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a2ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    source_lang, target_lang = create_dataset(path)\n",
    "    input_tensor, input_tokenizer = tokenize(source_lang)\n",
    "    target_tensor, target_tokenizer = tokenize(target_lang)\n",
    "    return input_tensor, target_tensor, input_tokenizer, target_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f532be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "input_tensor, target_tensor, input_tokenizer, target_tokenizer = load_dataset(\n",
    "    'data/Hindi_English_Truncated_Corpus.csv')\n",
    "max_input_len = max([len(x) for x in input_tensor])\n",
    "max_target_len = max([len(x) for x in target_tensor])\n",
    "\n",
    "max_input_len, max_target_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c687d",
   "metadata": {},
   "source": [
    "## Create train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "065b5ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31904, 23), (7977, 23), (31904, 32), (7977, 32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, \\\n",
    "target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor,\n",
    "                                                          test_size=0.2, random_state=42)\n",
    "input_tensor_train.shape, input_tensor_val.shape, \\\n",
    "target_tensor_train.shape, target_tensor_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cee6d",
   "metadata": {},
   "source": [
    "## Create TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc73772",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 32\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_size_input = len(input_tokenizer.word_index)+1\n",
    "vocab_size_target = len(target_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,\n",
    "                                              target_tensor_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e697ea1",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e788959",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a51ed01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units,\n",
    "                batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(self.enc_units,\n",
    "                       return_sequences=True,\n",
    "                       return_state=True,\n",
    "                       recurrent_initializer=\"glorot_uniform\")\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))\n",
    "    \n",
    "encoder = Encoder(vocab_size_input, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f06427",
   "metadata": {},
   "source": [
    "### Attention Mechanism (BahdaunauAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f73c1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3a15d",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca3ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units,\n",
    "                 batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units,\n",
    "                       return_sequences=True,\n",
    "                       return_state=True,\n",
    "                       recurrent_initializer=\"glorot_uniform\")\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden,\n",
    "                                                           enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(vocab_size_target, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1b36d",
   "metadata": {},
   "source": [
    "## Define optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0349ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                         reduction='none')\n",
    "\n",
    "def loss_function(true, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
    "    loss_ = loss_object(true, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "add077c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoints\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a9ac24",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c033d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']]*BATCH_SIZE, 1)\n",
    "        \n",
    "        # teacher forcing method\n",
    "        for i in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, i], 1)\n",
    "            \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return batch_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abcde7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.8431\n",
      "Epoch 1 Batch 100 Loss 2.0377\n",
      "Epoch 1 Batch 200 Loss 1.7499\n",
      "Epoch 1 Batch 300 Loss 1.8334\n",
      "Epoch 1 Batch 400 Loss 1.9309\n",
      "Epoch 1 Batch 500 Loss 1.8146\n",
      "Epoch 1 Batch 600 Loss 2.0090\n",
      "Epoch 1 Batch 700 Loss 1.7578\n",
      "Epoch 1 Batch 800 Loss 1.9163\n",
      "Epoch 1 Batch 900 Loss 1.9670\n",
      "Epoch 1 Loss 1.9109\n",
      "Time taken for 1 epoch 581.5377941131592 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7961\n",
      "Epoch 2 Batch 100 Loss 1.8769\n",
      "Epoch 2 Batch 200 Loss 1.9683\n",
      "Epoch 2 Batch 300 Loss 2.0691\n",
      "Epoch 2 Batch 400 Loss 2.1273\n",
      "Epoch 2 Batch 500 Loss 1.8904\n",
      "Epoch 2 Batch 600 Loss 2.1055\n",
      "Epoch 2 Batch 700 Loss 1.8803\n",
      "Epoch 2 Batch 800 Loss 2.0204\n",
      "Epoch 2 Batch 900 Loss 1.8793\n",
      "Epoch 2 Loss 1.8916\n",
      "Time taken for 1 epoch 915.207731962204 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch+1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "            \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"Epoch {} Loss {:.4f}\".format(epoch+1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print(\"Time taken for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecc6f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_target_len, max_input_len))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [input_tokenizer.word_index[i] for i in sentence.split()]\n",
    "    inputs = pad_sequences([inputs], maxlen=max_input_len, padding=\"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_output, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
    "    for i in range(max_target_len):\n",
    "        predictions, decc_hidden, attention_weights = decoder(dec_input, dec_hidden,\n",
    "                                                              enc_output)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += target_tokenizer.index_word[predicted_id] + ' '\n",
    "        if target_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cb67551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Translated {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c50884c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: politicians do not have permission to do what needs to be done\n",
      "Translated और और और और और और और और और और और और और और और और और और और और और और और और और और और और और और और और \n"
     ]
    }
   ],
   "source": [
    "translate(u'politicians do not have permission to do what needs to be done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ece07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64946e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
